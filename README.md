# MedGemma-Neuro MLOps System

**Production-Grade Medical AI System with RAG Architecture**

End-to-end MLOps implementation of a specialized medical LLM deployed on GCP for clinical decision support in Neurology.

## ðŸŽ¯ Project Overview

Designed and deployed a production medical AI system combining LoRA fine-tuning with RAG to mitigate hallucinations and ensure reliable responses across medical domains.

**Key Achievement:** Eliminated specialization bias while maintaining domain expertise through hybrid RAG architecture with <100ms latency.

## ðŸ›  Tech Stack

### Model Development
- **Base Model:** MedGemma 4B (Medical domain pre-trained)
- **Fine-tuning:** LoRA adaptation for Neurology specialization
- **Training Platform:** Vertex AI Training

### Production Infrastructure (GCP)
- **Vertex AI Endpoints:** Scalable model serving
- **Vertex AI Matching Engine:** Vector search for real-time RAG
- **Cloud Run:** Inference pipeline orchestration
- **BigQuery:** Logging and analytics

### MLOps Pipeline
- **Monitoring:** Vertex AI Model Monitoring (drift detection)
- **Observability:** Latency, throughput, accuracy metrics
- **CI/CD:** Automated model versioning and deployment

## ðŸ— Architecture

User Query â†’ Cloud Run â†’ [Matching Engine (RAG Context) + Fine-tuned Model] â†’ Response
â†“
BigQuery Logs â†’ Vertex AI Monitoring â†’ Drift Alerts


## ðŸ§  Technical Challenge Solved

**Problem:** Fine-tuning on Neurology induced specialization biasâ€”model hallucinated on general medicine queries (e.g., renal pathologies).

**Solution:** Hybrid RAG architecture:
1. Vertex AI Matching Engine indexes general medical knowledge base
2. Runtime retrieval augments specialized model with relevant context
3. Model processes: `[Query + RAG Context] â†’ Grounded Response`

**Result:** Zero hallucinations on out-of-domain queries while preserving Neurology expertise.

## ðŸ“Š Performance Metrics

- **Latency:** <100ms (p95)
- **Availability:** 99.9% uptime
- **Drift Detection:** Automated monitoring with alerting

## ðŸš€ MLOps Lifecycle

1. **Training:** LoRA fine-tuning on Vertex AI
2. **Deployment:** Versioned endpoints with blue-green strategy
3. **Monitoring:** Real-time drift detection and performance tracking
4. **Iteration:** Feedback loop from BigQuery analytics

---

**Role:** ML Engineer / MLOps  
**Focus:** Production systems, cloud infrastructure, model lifecycle management
